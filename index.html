<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A compact neuromorphic system for energy-efficient, on-device robot localization">
  <meta name="keywords" content="Visual place recognition, neuromorphic computing, spiking neural network">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LENS - Locational Encoding using Neuromorphic Systems</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/weblogo.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.qut.edu.au/research/centre-for-robotics">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Related work
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://vprtempo.github.io">
            VPRTempo
          </a>
          <a class="navbar-item" href="https://github.com/Tobias-Fischer/sparse-event-vpr">
            Sparse Event VPR
          </a>
          <a class="navbar-item" href="https://www.nature.com/articles/s41467-024-47811-6">
            SynSense SPECK
          </a>
          <a class="navbar-item" href="https://github.com/QVPR/VPRSNN">
            VPRSNN
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="./static/images/logoblack.png" alt="LENS Logo"
            style=" max-width: 80%; margin: auto;">
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="adamdhines.github.io">Adam D Hines</a>,</span>
            <span class="author-block">
              <a href="https://michaelmilford.com/">Michael Milford</a>,</span>
            <span class="author-block">
              <a href="https://www.tobiasfischer.info/">Tobias Fischer</a>,
            </span>
           
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">QUT Centre for Robotics, Brisbane QLD, Australia</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2408.16754"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Paper Link. -->
              <span class="link-block">
                <a href="https://doi.org/10.1234/your_paper" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-alt"></i>   <!-- or fa-solid fa-file-lines for FA6 -->
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/AdamDHines/LENS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-vcentered is-multiline">

      <!-- First video column -->
      <div class="column is-half has-text-centered">
        <h2 class="title is-5 mb-3">Real-time, event-driven localization</h2>
        <video autoplay muted loop playsinline style="width: 100%; border-radius: 6px;">
          <source src="./static/videos/lensvid.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>

      <!-- Second video column -->
      <div class="column is-half has-text-centered">
        <h2 class="title is-5 mb-3">
          Multi-environment on-robot deployment
        </h2>
        <video autoplay muted loop playsinline style="width: 100%; border-radius: 6px;">
          <source src="./static/videos/0808.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2 has-text-weight-bold mb-4">
          <span class="has-text-primary">LENS</span>
          <span class="is-block has-text-grey-dark" style="font-weight: normal; font-size: 1.3rem;">
            <span class="has-text-primary">L</span>ocational 
            <span class="has-text-primary">E</span>ncoding 
            with 
            <span class="has-text-primary">N</span>euromorphic 
            <span class="has-text-primary">S</span>ystems
          </span>
        </h2>
        <div class="content has-text-justified">
          <p>
            LENS is a compact, brain-inspired localization system for autonomous robots.
            Using spiking neural networks, dynamic vision sensors, and a neuromorphic processor on a single SPECK™ chip, 
            LENS performs real-time, event-driven place recognition with models 99% smaller and over 100× more energy-efficient than traditional systems. 
            Deployed on a Hexapod robot, it can learn and recognize over 700 places using fewer than 44k parameters—demonstrating the first large-scale, 
            fully event-driven localization on a mobile platform.
          </p>
        </div>
      </div>
    </div>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">SynSense SPECK™</h2>
          <p>
            Our system is a fully neuromorphic localization ecosystem
            developed for the SynSense SPECK™, combining an event camera and System-on-Chip
            neuromorphic processor.
          </p>
          <img id="speck" src="./static/images/IMG_3593.jpg" alt="SynSense SPECK in a 3D printed housing" height="100%">

        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Robotic Deployment</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              LENS is a fully capable system for deployment on resource constrained robotic platforms
              for multi-terrain, multi-environment mapping and localization.
            </p>
            <img id="speck" src="./static/images/IMG_3558.jpg" alt="Hexapod robot" height="100%">
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Performance</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Energy efficiency</h3>
        <div class="content has-text-justified">
          <p>
            When deployed on neuromorphic hardware, LENS uses <1% the power
            required by conventional compute platforms.
          </p>
          <img id="energy" src="./static/images/power.png" alt="Energy effeciency metrics" height="100%">
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <h3 class="title is-4">Localization performance</h3>
        <div class="content has-text-justified">
          <p>
            LENS shows impressive localization accuracy, outcompeting state-of-the-art VPR methods.
            With model sizes smaller than 180 KB with just 44k parameters able to map up to 8km.
          </p>
        </div>
        <div class="content has-text-centered">
          <img id="localization" src="./static/images/brisevent.png" alt="Localization performance" height="100%">
        </div>
        <!--/ Re-rendering. -->

        <h3 class="title is-3">Unique training</h3>
        <div class="content has-text-justified">
          <p>
            LENS trains on static DVS frames of events collected over a user-specified 
            timewindow. Temporal representations of event frames are efficiently trained
            in minutes for rapid deployment. Event counts create identifiable place representations
            through their unique, individual spiking patterns.
          </p>
        </div>
        <section class="lens vid">
          <div class="container is-max-desktop">
            <div class="hero-body">
              <video id="teaser1" autoplay muted loop playsinline height="100%">
                <source src="./static/videos/spiking_neural_network_two_examples.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </section>

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            This work was developed as an extension to a variety of excellent work in robotic localization.
          </p>
          <p>
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925670">Sparse event VPR</a> develops the concept of using small number of event pixels to perform accurate localization.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2311.13186">VPRSNN</a> introduced one of the first spiking neural networks for visual place recognition, 
            which inspired previous work for an efficiently trained and inferenced network <a href="https://arxiv.org/abs/2309.10225">VPRTempo</a>,
            which was adapted for this work.
          </p>
          <p>
            In addition to this, a lot of great work has been done in the localization and navigation field using neuromorphic hardware.
          </p>
          <p>
            <a href="https://www.science.org/doi/full/10.1126/scirobotics.abm6996">Fangwen Yu's work</a> developed an impressive multi-modal neural network for accurate place recognition.
            <a href="https://www.science.org/doi/10.1126/scirobotics.adg3679">Le Zhu</a> pioneered sequence learning using event cameras through vegetative environments.
            <a href="https://www.science.org/doi/10.1126/scirobotics.adk0310">Tom van Dijk</a> deployed an impressively compact neuromorphic system on a tiny autonomous drone for visual route following.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{hines2025lens,
      title={A compact neuromorphic system for ultra energy-efficient, on-device robot localization}, 
      author={Adam D. Hines and Michael Milford and Tobias Fischer},
      journal={},
      year={2025},
      volume={},
      number={},
      doi={},
      url={}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Please see the following for the source code of this website <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
